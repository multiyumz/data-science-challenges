{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter generation\n",
    "\n",
    "### Exercise objective\n",
    "- Get autonomous with Natural Language Processing\n",
    "- Generate Letters\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "In this exercise, we will try to generate some text. The underlying idea is, given a input sequence predict what the next letter is going to be. To do that, we will first create a dataset for this task, and then run a RNN to do the prediction.\n",
    "\n",
    "# The data\n",
    "\n",
    "❓ Question ❓ First, let's load the data. Here, it is the IMDB reviews again, but we are only interested in the sentences, not the positiveness or negativeness of the review. \n",
    "\n",
    "⚠️ **Warning** ⚠️ The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "**At the end of the notebook, to improve the model, you would maybe need to increase the number of loaded sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    # Load the data\n",
    "    (sentences_train, y_train), (sentences_test, y_test) = imdb.load_data()\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(sentences_train))\n",
    "        sentences_train = sentences_train[:len_train]\n",
    "        y_train = y_train[:len_train]\n",
    "        \n",
    "        len_test = int(percentage_of_sentences/100*len(sentences_test))\n",
    "        sentences_test = sentences_test[:len_test]\n",
    "        y_test = y_test[:len_test]\n",
    "            \n",
    "    # Load the {interger: word} representation\n",
    "    word_to_id = imdb.get_word_index()\n",
    "    word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "    for i, w in enumerate(['<PAD>', '<START>', '<UNK>', '<UNUSED>']):\n",
    "        word_to_id[w] = i\n",
    "\n",
    "    id_to_word = {v:k for k, v in word_to_id.items()}\n",
    "\n",
    "    # Convert the list of integers to list of words (str)\n",
    "    X_train = [' '.join([id_to_word[_] for _ in sentence[1:]]) for sentence in sentences_train]\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "\n",
    "### Just run this cell to load the data\n",
    "X = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function that, given a string (list of letters), returns\n",
    "- a string (list of letters) that corresponds to part of the sentence - this string should be of size 300\n",
    "- the letter that follow the previous string\n",
    "\n",
    "❗ **Remark** ❗ There is no reason your first strings to start at the beginning of the input string.\n",
    "\n",
    "Example:\n",
    "- Input : 'This is a good movie\"\n",
    "- Output: ('a good m', 'o') [Except the first part should be of size 300 instead of 8]\n",
    "\n",
    "❗ **Remark** ❗ If the input is shorter than 300 letters, return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_X_y(string, length=300):\n",
    "    if len(string) <= length:\n",
    "        return None\n",
    "    \n",
    "    first_letter_idx = np.random.randint(0, len(string) - length)\n",
    "    \n",
    "    X_letters = string[first_letter_idx:first_letter_idx+length]\n",
    "    y_letter = string[first_letter_idx+length]\n",
    "    \n",
    "    return X_letters, y_letter\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Check that the function is working on some strings from the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"worst mistake of my life br br i picked this movie up at target for 5 because i figured hey it's sandler i can get some cheap laughs i was wrong completely wrong mid way through the film all three of my friends were asleep and i was still suffering worst plot worst script worst movie i have ever seen i wanted to hit my head up against a wall for an hour then i'd stop and you know why because it felt damn good upon bashing my head in i stuck that damn movie in the microwave and watched it burn and that felt better than anything else i've ever done it took american psycho army of darkness and kill bill just to get over that crap i hate you sandler for actually going through with this and ruining a whole day of my life\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_X_y(X[3])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Write a function, that, based on the previous function and the loaded sentences, generate a dataset X and y:\n",
    "- each sample of X is a string\n",
    "- the corresponding y is the letter that comes just after in the input string\n",
    "\n",
    "❗ **Remark** ❗ This question is not much guided as it is similar to what you have done in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def create_dataset(sentences):\n",
    "    X, y = [], []\n",
    "    number_of_samples = 20000\n",
    "    indicies = np.random.randint(0, len(sentences), size=number_of_samples)\n",
    "    \n",
    "    for idx in indicies:\n",
    "        ret = get_X_y(sentences[idx])\n",
    "        if ret is None:\n",
    "            continue\n",
    "        xi, yi = ret\n",
    "        \n",
    "        X.append(xi)\n",
    "        y.append(yi)\n",
    "    \n",
    "    return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_dataset(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Split X and y in train and test data. Store it in `string_train`, `string_test`, `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "string_train, string_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Create a dictionary which stores a unique token for each letter: the key is the letter while the value is the corresponding token. You have to build you dictionary based on the letters that are in `string_train` and `y_train` only, as you are not supposed to know the test set (and the new letters that might appear, which is unlikely, but still possible).\n",
    "\n",
    "❗ **Remark** ❗ To account for the fact that there might be letters in the test set that are not in the train set, add a particular token for that, whose corresponding key can be `UNKNOWN`.\n",
    "\n",
    "❗ **Remark** ❗ By letter, we actually mean any character. As there happen to be numbers (`1`, `2`, ...) or `?`, `!`, `@`, ... in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "letter_to_id = {}\n",
    "letter_to_id['UNKNOWN'] = 0\n",
    "\n",
    "iter_ = 1\n",
    "\n",
    "for string in string_train:\n",
    "    for letter in string:\n",
    "        if letter in letter_to_id:\n",
    "            continue\n",
    "        letter_to_id[letter] = iter_\n",
    "        iter_ += 1\n",
    "    \n",
    "for string in y_train:\n",
    "    for letter in string:\n",
    "        if letter in letter_to_id:\n",
    "            continue\n",
    "        letter_to_id[letter] = iter_\n",
    "        iter_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNKNOWN': 0,\n",
       " 'n': 1,\n",
       " 'y': 2,\n",
       " 'o': 3,\n",
       " 'e': 4,\n",
       " ' ': 5,\n",
       " 'w': 6,\n",
       " 'a': 7,\n",
       " 't': 8,\n",
       " 'h': 9,\n",
       " \"'\": 10,\n",
       " 's': 11,\n",
       " 'i': 12,\n",
       " 'r': 13,\n",
       " 'l': 14,\n",
       " 'f': 15,\n",
       " 'g': 16,\n",
       " 'u': 17,\n",
       " 'p': 18,\n",
       " 'v': 19,\n",
       " 'd': 20,\n",
       " 'm': 21,\n",
       " 'b': 22,\n",
       " 'c': 23,\n",
       " 'k': 24,\n",
       " '2': 25,\n",
       " '0': 26,\n",
       " '3': 27,\n",
       " 'z': 28,\n",
       " 'j': 29,\n",
       " 'x': 30,\n",
       " '1': 31,\n",
       " '9': 32,\n",
       " '4': 33,\n",
       " 'q': 34,\n",
       " '5': 35,\n",
       " 'é': 36,\n",
       " '7': 37,\n",
       " '8': 38,\n",
       " 'è': 39,\n",
       " '6': 40,\n",
       " '\\x96': 41,\n",
       " '\\x85': 42,\n",
       " '´': 43,\n",
       " 'ä': 44,\n",
       " 'ï': 45,\n",
       " 'ç': 46,\n",
       " 'ã': 47,\n",
       " 'ö': 48,\n",
       " '–': 49,\n",
       " '\\x91': 50,\n",
       " '“': 51,\n",
       " '’': 52,\n",
       " '”': 53,\n",
       " 'ü': 54,\n",
       " 'ó': 55,\n",
       " '\\x97': 56,\n",
       " 'í': 57,\n",
       " 'ñ': 58,\n",
       " 'å': 59,\n",
       " 'á': 60,\n",
       " '\\xa0': 61,\n",
       " 'à': 62,\n",
       " '\\x95': 63,\n",
       " '£': 64}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letter_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Based on the previous dictionary, tokenize the strings and store them in `X_train` and `X_tests`.\n",
    "\n",
    "❗ **Remark** ❗ Convert your lists to NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = [[letter_to_id[_] for _ in x] for x in string_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[letter_to_id[_] if _ in letter_to_id else letter_to_id['UNKNOWN'] for _ in x] for x in string_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3, ...,  5,  9,  7],\n",
       "       [13,  5,  6, ...,  5, 20,  4],\n",
       "       [ 4,  8,  5, ...,  4,  2,  5],\n",
       "       ...,\n",
       "       [ 5,  8, 19, ...,  1, 12,  7],\n",
       "       [ 5, 15, 12, ...,  8,  5, 34],\n",
       "       [19,  4,  5, ...,  4,  7,  8]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13477, 300)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for x in string_train:\n",
    "    lst.append([letter_to_id[letter] for letter in x])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ The outputs are currently letters. We first need to tokenize them, thanks to the previous dictionary.\n",
    "\n",
    "❗ **Remark** ❗ Remember that some values in `y_test` are maybe unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "y_train_token = [letter_to_id[x] for x in y_train]\n",
    "y_test_token = [letter_to_id[x] if x in letter_to_id else letter_to_id['UNKNOWN'] for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, let's convert the tokenized outputs to one-hot encoded categories! There should be as many categories as different letters in the previous dictionary! So be careful that your outputs are of the right shape, especially as many one-hot encoded categories in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train_token, num_classes=len(letter_to_id))\n",
    "y_test_cat = to_categorical(y_test_token, num_classes=len(letter_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 13,\n",
       " 14,\n",
       " 12,\n",
       " 5,\n",
       " 14,\n",
       " 16,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 14,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 12,\n",
       " 2,\n",
       " 8,\n",
       " 4,\n",
       " 21,\n",
       " 5,\n",
       " 20,\n",
       " 5,\n",
       " 12,\n",
       " 18,\n",
       " 5,\n",
       " 14,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 17,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 20,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 5,\n",
       " 24,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 20,\n",
       " 14,\n",
       " 12,\n",
       " 11,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 17,\n",
       " 3,\n",
       " 11,\n",
       " 5,\n",
       " 30,\n",
       " 9,\n",
       " 4,\n",
       " 11,\n",
       " 23,\n",
       " 14,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 16,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 11,\n",
       " 17,\n",
       " 11,\n",
       " 4,\n",
       " 8,\n",
       " 17,\n",
       " 5,\n",
       " 4,\n",
       " 12,\n",
       " 17,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 14,\n",
       " 5,\n",
       " 14,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 13,\n",
       " 13,\n",
       " 1,\n",
       " 21,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 9,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 11,\n",
       " 5,\n",
       " 16,\n",
       " 12,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 11,\n",
       " 13,\n",
       " 15,\n",
       " 6,\n",
       " 17,\n",
       " 1,\n",
       " 20,\n",
       " 16,\n",
       " 5,\n",
       " 1,\n",
       " 14,\n",
       " 1,\n",
       " 8,\n",
       " 22,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 15,\n",
       " 6,\n",
       " 21,\n",
       " 5,\n",
       " 2,\n",
       " 13,\n",
       " 5,\n",
       " 23,\n",
       " 1,\n",
       " 9,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 4,\n",
       " 9,\n",
       " 18,\n",
       " 7,\n",
       " 11,\n",
       " 5,\n",
       " 14,\n",
       " 12,\n",
       " 3,\n",
       " 4,\n",
       " 12,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 23,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 14,\n",
       " 11,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 13,\n",
       " 15,\n",
       " 5,\n",
       " 13,\n",
       " 12,\n",
       " 8,\n",
       " 4,\n",
       " 11,\n",
       " 8,\n",
       " 20,\n",
       " 4,\n",
       " 13,\n",
       " 14,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 11,\n",
       " 23,\n",
       " 17,\n",
       " 22,\n",
       " 5,\n",
       " 20,\n",
       " 5,\n",
       " 8,\n",
       " 11,\n",
       " 3,\n",
       " 13,\n",
       " 21,\n",
       " 5,\n",
       " 15,\n",
       " 9,\n",
       " 24,\n",
       " 8,\n",
       " 3,\n",
       " 12,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 13,\n",
       " 20,\n",
       " 20,\n",
       " 11,\n",
       " 8,\n",
       " 24,\n",
       " 13,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 3,\n",
       " 13,\n",
       " 20,\n",
       " 12,\n",
       " 9,\n",
       " 13,\n",
       " 4,\n",
       " 1,\n",
       " 8,\n",
       " 11,\n",
       " 5,\n",
       " 1,\n",
       " 20,\n",
       " 12,\n",
       " 5,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 12,\n",
       " 4,\n",
       " 12,\n",
       " 1,\n",
       " 21,\n",
       " 3,\n",
       " 8,\n",
       " 2,\n",
       " 13,\n",
       " 5,\n",
       " 11,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 4,\n",
       " 11,\n",
       " 7,\n",
       " 3,\n",
       " 20,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 11,\n",
       " 12,\n",
       " 23,\n",
       " 13,\n",
       " 13,\n",
       " 4,\n",
       " 7,\n",
       " 20,\n",
       " 12,\n",
       " 8,\n",
       " 23,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 9,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 7,\n",
       " 1,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 21,\n",
       " 14,\n",
       " 22,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 14,\n",
       " 21,\n",
       " 17,\n",
       " 5,\n",
       " 9,\n",
       " 8,\n",
       " 23,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 9,\n",
       " 13,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 23,\n",
       " 16,\n",
       " 3,\n",
       " 14,\n",
       " 15,\n",
       " 12,\n",
       " 9,\n",
       " 8,\n",
       " 11,\n",
       " 20,\n",
       " 3,\n",
       " 7,\n",
       " 15,\n",
       " 11,\n",
       " 3,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 22,\n",
       " 13,\n",
       " 12,\n",
       " 3,\n",
       " 1,\n",
       " 7,\n",
       " 2,\n",
       " 5,\n",
       " 20,\n",
       " 21,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 23,\n",
       " 8,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 20,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 17,\n",
       " 3,\n",
       " 4,\n",
       " 20,\n",
       " 5,\n",
       " 1,\n",
       " 11,\n",
       " 5,\n",
       " 13,\n",
       " 7,\n",
       " 3,\n",
       " 4,\n",
       " 17,\n",
       " 20,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 3,\n",
       " 13,\n",
       " 11,\n",
       " 9,\n",
       " 12,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 14,\n",
       " 5,\n",
       " 22,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 4,\n",
       " 23,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 14,\n",
       " 4,\n",
       " 20,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 7,\n",
       " 11,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 17,\n",
       " 4,\n",
       " 5,\n",
       " 29,\n",
       " 3,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 12,\n",
       " 12,\n",
       " 22,\n",
       " 1,\n",
       " 7,\n",
       " 12,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 13,\n",
       " 11,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 20,\n",
       " 21,\n",
       " 4,\n",
       " 7,\n",
       " 14,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 13,\n",
       " 11,\n",
       " 21,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 19,\n",
       " 12,\n",
       " 23,\n",
       " 12,\n",
       " 5,\n",
       " 11,\n",
       " 4,\n",
       " 23,\n",
       " 8,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 12,\n",
       " 11,\n",
       " 3,\n",
       " 19,\n",
       " 1,\n",
       " 8,\n",
       " 13,\n",
       " 8,\n",
       " 4,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 16,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 19,\n",
       " 5,\n",
       " 14,\n",
       " 9,\n",
       " 11,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 11,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 4,\n",
       " 11,\n",
       " 3,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 11,\n",
       " 1,\n",
       " 6,\n",
       " 2,\n",
       " 20,\n",
       " 14,\n",
       " 5,\n",
       " 12,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 17,\n",
       " 4,\n",
       " 3,\n",
       " 15,\n",
       " 5,\n",
       " 16,\n",
       " 7,\n",
       " 5,\n",
       " 12,\n",
       " 5,\n",
       " 13,\n",
       " 13,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 22,\n",
       " 23,\n",
       " 1,\n",
       " 3,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 15,\n",
       " 5,\n",
       " 12,\n",
       " 4,\n",
       " 13,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 18,\n",
       " 4,\n",
       " 1,\n",
       " 12,\n",
       " 5,\n",
       " 3,\n",
       " 20,\n",
       " 20,\n",
       " 11,\n",
       " 11,\n",
       " 24,\n",
       " 16,\n",
       " 11,\n",
       " 11,\n",
       " 4,\n",
       " 13,\n",
       " 1,\n",
       " 12,\n",
       " 3,\n",
       " 14,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 13,\n",
       " 20,\n",
       " 12,\n",
       " 1,\n",
       " 5,\n",
       " 13,\n",
       " 23,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 16,\n",
       " 23,\n",
       " 8,\n",
       " 4,\n",
       " 5,\n",
       " 8,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 8,\n",
       " 18,\n",
       " 11,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 12,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 14,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 12,\n",
       " 11,\n",
       " 11,\n",
       " 5,\n",
       " 19,\n",
       " 8,\n",
       " 2,\n",
       " 14,\n",
       " 5,\n",
       " 21,\n",
       " 20,\n",
       " 14,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 14,\n",
       " 7,\n",
       " 4,\n",
       " 13,\n",
       " 8,\n",
       " 1,\n",
       " 5,\n",
       " 34,\n",
       " 13,\n",
       " 4,\n",
       " 11,\n",
       " 5,\n",
       " 11,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 3,\n",
       " 7,\n",
       " 13,\n",
       " 3,\n",
       " 4,\n",
       " 8,\n",
       " 14,\n",
       " 6,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 6,\n",
       " 12,\n",
       " 15,\n",
       " 1,\n",
       " 21,\n",
       " 7,\n",
       " 23,\n",
       " 14,\n",
       " 6,\n",
       " 13,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 14,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 21,\n",
       " 18,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 14,\n",
       " 16,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 13,\n",
       " 6,\n",
       " 11,\n",
       " 5,\n",
       " 23,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 5,\n",
       " 20,\n",
       " 20,\n",
       " 3,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 11,\n",
       " 12,\n",
       " 5,\n",
       " 2,\n",
       " 16,\n",
       " 4,\n",
       " 20,\n",
       " 5,\n",
       " 7,\n",
       " 1,\n",
       " 14,\n",
       " 3,\n",
       " 12,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 11,\n",
       " 13,\n",
       " 20,\n",
       " 3,\n",
       " 12,\n",
       " 20,\n",
       " 13,\n",
       " 8,\n",
       " 20,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 18,\n",
       " 23,\n",
       " 12,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 21,\n",
       " 3,\n",
       " 5,\n",
       " 21,\n",
       " 5,\n",
       " 16,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 21,\n",
       " 5,\n",
       " 12,\n",
       " 2,\n",
       " 12,\n",
       " 3,\n",
       " 8,\n",
       " 1,\n",
       " 3,\n",
       " 23,\n",
       " 4,\n",
       " 19,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 17,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 13,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 16,\n",
       " 9,\n",
       " 4,\n",
       " 17,\n",
       " 7,\n",
       " 5,\n",
       " 1,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 13,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 17,\n",
       " 5,\n",
       " 7,\n",
       " 12,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 4,\n",
       " 9,\n",
       " 31,\n",
       " 5,\n",
       " 17,\n",
       " 15,\n",
       " 11,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 24,\n",
       " 5,\n",
       " 23,\n",
       " 12,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 4,\n",
       " 12,\n",
       " 19,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 20,\n",
       " 17,\n",
       " 5,\n",
       " 5,\n",
       " 18,\n",
       " 12,\n",
       " 7,\n",
       " 13,\n",
       " 5,\n",
       " 8,\n",
       " 14,\n",
       " 20,\n",
       " 18,\n",
       " 13,\n",
       " 13,\n",
       " 3,\n",
       " 7,\n",
       " 14,\n",
       " 7,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 13,\n",
       " 1,\n",
       " 19,\n",
       " 10,\n",
       " 13,\n",
       " 13,\n",
       " 2,\n",
       " 11,\n",
       " 23,\n",
       " 23,\n",
       " 22,\n",
       " 22,\n",
       " 4,\n",
       " 21,\n",
       " 11,\n",
       " 2,\n",
       " 13,\n",
       " 20,\n",
       " 1,\n",
       " 6,\n",
       " 1,\n",
       " 9,\n",
       " 5,\n",
       " 3,\n",
       " 7,\n",
       " 18,\n",
       " 8,\n",
       " 5,\n",
       " 3,\n",
       " 23,\n",
       " 7,\n",
       " 11,\n",
       " 1,\n",
       " 12,\n",
       " 11,\n",
       " 13,\n",
       " 9,\n",
       " 14,\n",
       " 3,\n",
       " 21,\n",
       " 21,\n",
       " 7,\n",
       " 13,\n",
       " 13,\n",
       " 8,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 20,\n",
       " 5,\n",
       " 5,\n",
       " 15,\n",
       " 17,\n",
       " 7,\n",
       " 7,\n",
       " 16,\n",
       " 21,\n",
       " 7,\n",
       " 5,\n",
       " 23,\n",
       " 9,\n",
       " 5,\n",
       " 1,\n",
       " 11,\n",
       " 18,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 12,\n",
       " ...]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model\n",
    "\n",
    "❓ **Question** ❓ What is the baseline accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of labels in the train set  {' ': 2516, \"'\": 54, '0': 10, '1': 8, '2': 2, '3': 3, '4': 2, '5': 2, '6': 2, '7': 1, '8': 2, '9': 4, 'a': 862, 'b': 220, 'c': 315, 'd': 421, 'e': 1212, 'f': 215, 'g': 206, 'h': 579, 'i': 855, 'j': 28, 'k': 91, 'l': 476, 'm': 326, 'n': 731, 'o': 789, 'p': 193, 'q': 7, 'r': 666, 's': 705, 't': 1077, 'u': 277, 'v': 138, 'w': 228, 'x': 23, 'y': 218, 'z': 11, '\\x96': 1, 'ö': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "counts = dict(zip(unique, counts))\n",
    "\n",
    "print(\"The number of labels in the train set \", counts)\n",
    "    \n",
    "w = -1\n",
    "y_pred = ''\n",
    "for k, v in counts.items():\n",
    "    if v > w:\n",
    "        y_pred = k\n",
    "        w = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy:  0.19006404708326122\n"
     ]
    }
   ],
   "source": [
    "print(f'Baseline accuracy: ', accuracy_score(y_test, [y_pred]*len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model\n",
    "\n",
    "❓ **Question** ❓ Write a RNN with all the appropriate layers, and compile it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, None, 30)          1950      \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 30)                5580      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 65)                2015      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,475\n",
      "Trainable params: 10,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential, layers\n",
    "\n",
    "def init_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=30))\n",
    "    model.add(layers.GRU(30, activation='tanh'))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dense(vocab_size, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = init_model(len(letter_to_id))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Fit the model - you can use a large batch size to accelerate the convergence. The model will probably hit the baseline performance at some point, and hopefully keep decreasing from here. \n",
    "\n",
    "You should get an accuracy better than 35% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "189/189 [==============================] - 24s 109ms/step - loss: 3.0716 - accuracy: 0.1887 - val_loss: 2.7967 - val_accuracy: 0.2136\n",
      "Epoch 2/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.6844 - accuracy: 0.2311 - val_loss: 2.5982 - val_accuracy: 0.2658\n",
      "Epoch 3/400\n",
      "189/189 [==============================] - 21s 109ms/step - loss: 2.4992 - accuracy: 0.2762 - val_loss: 2.4837 - val_accuracy: 0.2883\n",
      "Epoch 4/400\n",
      "189/189 [==============================] - 20s 105ms/step - loss: 2.4104 - accuracy: 0.3001 - val_loss: 2.4250 - val_accuracy: 0.2933\n",
      "Epoch 5/400\n",
      "189/189 [==============================] - 20s 105ms/step - loss: 2.3615 - accuracy: 0.3045 - val_loss: 2.3875 - val_accuracy: 0.3027\n",
      "Epoch 6/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 2.3247 - accuracy: 0.3092 - val_loss: 2.3533 - val_accuracy: 0.3106\n",
      "Epoch 7/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 2.2958 - accuracy: 0.3135 - val_loss: 2.3293 - val_accuracy: 0.3103\n",
      "Epoch 8/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.2700 - accuracy: 0.3184 - val_loss: 2.3134 - val_accuracy: 0.3210\n",
      "Epoch 9/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.2460 - accuracy: 0.3229 - val_loss: 2.2965 - val_accuracy: 0.3239\n",
      "Epoch 10/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 2.2246 - accuracy: 0.3311 - val_loss: 2.2729 - val_accuracy: 0.3269\n",
      "Epoch 11/400\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 2.2031 - accuracy: 0.3340 - val_loss: 2.2568 - val_accuracy: 0.3296\n",
      "Epoch 12/400\n",
      "189/189 [==============================] - 19s 101ms/step - loss: 2.1832 - accuracy: 0.3427 - val_loss: 2.2422 - val_accuracy: 0.3403\n",
      "Epoch 13/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.1655 - accuracy: 0.3468 - val_loss: 2.2302 - val_accuracy: 0.3427\n",
      "Epoch 14/400\n",
      "189/189 [==============================] - 20s 105ms/step - loss: 2.1492 - accuracy: 0.3505 - val_loss: 2.2108 - val_accuracy: 0.3482\n",
      "Epoch 15/400\n",
      "189/189 [==============================] - 19s 103ms/step - loss: 2.1326 - accuracy: 0.3562 - val_loss: 2.2022 - val_accuracy: 0.3521\n",
      "Epoch 16/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 2.1161 - accuracy: 0.3618 - val_loss: 2.1944 - val_accuracy: 0.3526\n",
      "Epoch 17/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 2.1005 - accuracy: 0.3625 - val_loss: 2.1863 - val_accuracy: 0.3600\n",
      "Epoch 18/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.0864 - accuracy: 0.3701 - val_loss: 2.1736 - val_accuracy: 0.3660\n",
      "Epoch 19/400\n",
      "189/189 [==============================] - 19s 103ms/step - loss: 2.0714 - accuracy: 0.3754 - val_loss: 2.1683 - val_accuracy: 0.3702\n",
      "Epoch 20/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 2.0573 - accuracy: 0.3762 - val_loss: 2.1621 - val_accuracy: 0.3704\n",
      "Epoch 21/400\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 2.0422 - accuracy: 0.3820 - val_loss: 2.1558 - val_accuracy: 0.3709\n",
      "Epoch 22/400\n",
      "189/189 [==============================] - 19s 102ms/step - loss: 2.0286 - accuracy: 0.3863 - val_loss: 2.1537 - val_accuracy: 0.3727\n",
      "Epoch 23/400\n",
      "189/189 [==============================] - 19s 103ms/step - loss: 2.0153 - accuracy: 0.3909 - val_loss: 2.1445 - val_accuracy: 0.3756\n",
      "Epoch 24/400\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 2.0033 - accuracy: 0.3948 - val_loss: 2.1371 - val_accuracy: 0.3811\n",
      "Epoch 25/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.9903 - accuracy: 0.3986 - val_loss: 2.1352 - val_accuracy: 0.3791\n",
      "Epoch 26/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.9775 - accuracy: 0.4040 - val_loss: 2.1287 - val_accuracy: 0.3823\n",
      "Epoch 27/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 1.9655 - accuracy: 0.4038 - val_loss: 2.1282 - val_accuracy: 0.3823\n",
      "Epoch 28/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 1.9536 - accuracy: 0.4076 - val_loss: 2.1186 - val_accuracy: 0.3850\n",
      "Epoch 29/400\n",
      "189/189 [==============================] - 20s 106ms/step - loss: 1.9418 - accuracy: 0.4097 - val_loss: 2.1272 - val_accuracy: 0.3867\n",
      "Epoch 30/400\n",
      "189/189 [==============================] - 19s 103ms/step - loss: 1.9321 - accuracy: 0.4139 - val_loss: 2.1204 - val_accuracy: 0.3877\n",
      "Epoch 31/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.9207 - accuracy: 0.4154 - val_loss: 2.1159 - val_accuracy: 0.3917\n",
      "Epoch 32/400\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 1.9097 - accuracy: 0.4193 - val_loss: 2.1141 - val_accuracy: 0.3902\n",
      "Epoch 33/400\n",
      "189/189 [==============================] - 20s 107ms/step - loss: 1.9003 - accuracy: 0.4220 - val_loss: 2.1151 - val_accuracy: 0.3917\n",
      "Epoch 34/400\n",
      "189/189 [==============================] - 20s 108ms/step - loss: 1.8876 - accuracy: 0.4252 - val_loss: 2.1108 - val_accuracy: 0.3947\n",
      "Epoch 35/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.8800 - accuracy: 0.4280 - val_loss: 2.1136 - val_accuracy: 0.3912\n",
      "Epoch 36/400\n",
      "189/189 [==============================] - 21s 110ms/step - loss: 1.8703 - accuracy: 0.4304 - val_loss: 2.1062 - val_accuracy: 0.3984\n",
      "Epoch 37/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.8595 - accuracy: 0.4305 - val_loss: 2.1121 - val_accuracy: 0.3937\n",
      "Epoch 38/400\n",
      "189/189 [==============================] - 20s 104ms/step - loss: 1.8512 - accuracy: 0.4374 - val_loss: 2.1161 - val_accuracy: 0.3939\n",
      "Epoch 39/400\n",
      "189/189 [==============================] - 19s 102ms/step - loss: 1.8422 - accuracy: 0.4389 - val_loss: 2.1079 - val_accuracy: 0.3996\n",
      "Epoch 40/400\n",
      "189/189 [==============================] - 19s 102ms/step - loss: 1.8335 - accuracy: 0.4425 - val_loss: 2.1127 - val_accuracy: 0.3964\n",
      "Epoch 41/400\n",
      "189/189 [==============================] - 19s 103ms/step - loss: 1.8249 - accuracy: 0.4463 - val_loss: 2.1153 - val_accuracy: 0.4038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16af0fca0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, monitor='val_loss')\n",
    "\n",
    "model = init_model(len(letter_to_id))\n",
    "\n",
    "model.fit(X_train, y_train_cat,\n",
    "          epochs=400, \n",
    "          batch_size=50,\n",
    "          callbacks=[es],\n",
    "          validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Evaluate your model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181/181 [==============================] - 2s 12ms/step - loss: 2.1449 - accuracy: 0.3869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.144914150238037, 0.38687899708747864]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Even though the model is not perfect, you can look at its prediction with a string of your choice. Don't forget to decode the predicted token to know which letter it corresponds to.\n",
    "\n",
    "You will have to convert your input string to a list of tokens, get the most probable output class, and then convert it back to a letter.\n",
    "\n",
    "You should do it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "id_to_letter = {v: k for k, v in letter_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_letter(string):\n",
    "    string_convert = [letter_to_id[_] for _ in string]\n",
    "    \n",
    "    pred = model.predict([string_convert])\n",
    "    pred_class = np.argmax(pred[0])\n",
    "    pred_letter = id_to_letter[pred_class]\n",
    " \n",
    "    return pred_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_predicted_letter('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, write a function that takes a string as an input, predicts the next letter, appends the letter to the initial string, then redoes the prediction, etc etc.\n",
    "\n",
    "For instance : \n",
    "- 'this is a good' => ' '\n",
    "- 'this is a good ' => 'm'\n",
    "- 'this is a good m' => 'o'\n",
    "...\n",
    "\n",
    "The function should also take the number of times you repeat the operation as an input.\n",
    "\n",
    "You can have some fun trying different input sequences here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "def repeat_prediction(string, repetition):\n",
    "    string_tmp = string\n",
    "    \n",
    "    for i in range(repetition):\n",
    "        predicted_letter = get_predicted_letter(string_tmp)\n",
    "        string_tmp = string_tmp + predicted_letter\n",
    "        \n",
    "    return string_tmp    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ['want i like']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['want i like the the t']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[repeat_prediction(string, 10) for string in strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Try to optimize your architecture to improve your performance. You can also try to load more data in the first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
