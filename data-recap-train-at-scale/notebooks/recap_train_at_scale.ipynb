{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap train at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bingobango/code/lewagon/data-recap-train-at-scale/taxifare/__init__.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, check that you run this notebook with the correct taxifare-env kernel\n",
    "import taxifare\n",
    "taxifare.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should be able to load the following files\n",
    "import os\n",
    "from taxifare.params import *\n",
    "data_processed_path_200k = os.path.join(LOCAL_DATA_PATH, \"processed\",\"processed_2009-01-01_2015-01-01_200k.csv\")\n",
    "data_processed_path_all = os.path.join(LOCAL_DATA_PATH, \"processed\",\"processed_2009-01-01_2015-01-01_all.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary markdown='span'>If files are missings</summary>\n",
    "\n",
    "```bash\n",
    "make reset_local_files_with_csv_solutions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Explain concepts of incremental fit by chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/07-ML-OPS/train_by_chunk.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Explain code solution for `main_local.train()`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(min_date:str = '2009-01-01', max_date:str = '2015-01-01') -> None:\n",
    "    \"\"\"\n",
    "    Incremental train on the (already preprocessed) dataset locally stored.\n",
    "    - Loading data chunk-by-chunk\n",
    "    - Updating the weight of the model for each chunk\n",
    "    - Saving validation metrics at each chunks, and final model weights on local disk\n",
    "    \"\"\"\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's launch a training by batch on 200k rows! (set DATA_SIZE='200k' in params.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 21:36:12.082131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Loading TensorFlow...\u001b[0m\n",
      "\n",
      "✅ TensorFlow loaded (0.0s)\n",
      "\u001b[35m\n",
      " ⭐️ Use case: train in batches\u001b[0m\n",
      "Training on preprocessed chunk n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-18 21:36:19.739861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model initialized\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 10: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 8.53\n",
      "8.527969360351562\n",
      "Training on preprocessed chunk n°1\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 9.59\n",
      "9.591536521911621\n",
      "Training on preprocessed chunk n°2\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 56.\n",
      "Epoch 58: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 7.65\n",
      "7.649648666381836\n",
      "Training on preprocessed chunk n°3\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 6.14\n",
      "6.135122299194336\n",
      "Training on preprocessed chunk n°4\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 7.69\n",
      "7.686532497406006\n",
      "Training on preprocessed chunk n°5\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 17: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 6.7\n",
      "6.696028232574463\n",
      "Training on preprocessed chunk n°6\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 6.62\n",
      "6.620745658874512\n",
      "Training on preprocessed chunk n°7\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 39.\n",
      "Epoch 41: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 4.87\n",
      "4.8719305992126465\n",
      "Training on preprocessed chunk n°8\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 24.\n",
      "Epoch 26: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 4.39\n",
      "4.390707492828369\n",
      "Training on preprocessed chunk n°9\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.79\n",
      "2.7897167205810547\n",
      "Training on preprocessed chunk n°10\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 3.64\n",
      "3.641197443008423\n",
      "Training on preprocessed chunk n°11\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 17: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.42\n",
      "2.4185633659362793\n",
      "Training on preprocessed chunk n°12\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.96\n",
      "2.961346387863159\n",
      "Training on preprocessed chunk n°13\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.48\n",
      "2.482435703277588\n",
      "Training on preprocessed chunk n°14\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.82\n",
      "2.8164873123168945\n",
      "Training on preprocessed chunk n°15\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 3.11\n",
      "3.108549118041992\n",
      "Training on preprocessed chunk n°16\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 17: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.0\n",
      "1.9970327615737915\n",
      "Training on preprocessed chunk n°17\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.23\n",
      "2.2281923294067383\n",
      "Training on preprocessed chunk n°18\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 8: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.06\n",
      "2.063314437866211\n",
      "Training on preprocessed chunk n°19\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 9.\n",
      "Epoch 11: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.87\n",
      "1.874481201171875\n",
      "Training on preprocessed chunk n°20\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 12: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.39\n",
      "2.390101909637451\n",
      "Training on preprocessed chunk n°21\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 9: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.95\n",
      "1.9455832242965698\n",
      "Training on preprocessed chunk n°22\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.15\n",
      "2.1484625339508057\n",
      "Training on preprocessed chunk n°23\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 9: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.24\n",
      "2.241554021835327\n",
      "Training on preprocessed chunk n°24\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "Epoch 29: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 5.0\n",
      "4.996834754943848\n",
      "Training on preprocessed chunk n°25\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 3.14\n",
      "3.135178804397583\n",
      "Training on preprocessed chunk n°26\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 11.\n",
      "Epoch 13: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.35\n",
      "2.34718656539917\n",
      "Training on preprocessed chunk n°27\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.0\n",
      "2.0031704902648926\n",
      "Training on preprocessed chunk n°28\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.54\n",
      "1.5389140844345093\n",
      "Training on preprocessed chunk n°29\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "Epoch 14: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.77\n",
      "1.7686065435409546\n",
      "Training on preprocessed chunk n°30\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 13.\n",
      "Epoch 15: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.85\n",
      "1.8498902320861816\n",
      "Training on preprocessed chunk n°31\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.94\n",
      "1.9426028728485107\n",
      "Training on preprocessed chunk n°32\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 6: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.75\n",
      "1.7481110095977783\n",
      "Training on preprocessed chunk n°33\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.26\n",
      "1.2572470903396606\n",
      "Training on preprocessed chunk n°34\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.29\n",
      "2.2927472591400146\n",
      "Training on preprocessed chunk n°35\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 3.19\n",
      "3.1916539669036865\n",
      "Training on preprocessed chunk n°36\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.26\n",
      "2.2572693824768066\n",
      "Training on preprocessed chunk n°37\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.88\n",
      "1.8847678899765015\n",
      "Training on preprocessed chunk n°38\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.0\n",
      "2.00361967086792\n",
      "Training on preprocessed chunk n°39\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.34\n",
      "2.337512493133545\n",
      "Training on preprocessed chunk n°40\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 8: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.1\n",
      "2.104429006576538\n",
      "Training on preprocessed chunk n°41\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.48\n",
      "2.4801855087280273\n",
      "Training on preprocessed chunk n°42\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.69\n",
      "1.6856977939605713\n",
      "Training on preprocessed chunk n°43\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.77\n",
      "1.7704273462295532\n",
      "Training on preprocessed chunk n°44\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 18 calls to <function Model.make_test_function.<locals>.test_function at 0x14fbd7d00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.62\n",
      "1.6151230335235596\n",
      "Training on preprocessed chunk n°45\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.98\n",
      "1.9806514978408813\n",
      "Training on preprocessed chunk n°46\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.81\n",
      "1.8101290464401245\n",
      "Training on preprocessed chunk n°47\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.97\n",
      "1.9749542474746704\n",
      "Training on preprocessed chunk n°48\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "WARNING:tensorflow:5 out of the last 14 calls to <function Model.make_test_function.<locals>.test_function at 0x150b168c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.34\n",
      "2.343687057495117\n",
      "Training on preprocessed chunk n°49\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.78\n",
      "1.784235954284668\n",
      "Training on preprocessed chunk n°50\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.96\n",
      "2.9611921310424805\n",
      "Training on preprocessed chunk n°51\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 10.\n",
      "Epoch 12: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.95\n",
      "1.9542728662490845\n",
      "Training on preprocessed chunk n°52\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.12\n",
      "2.124274253845215\n",
      "Training on preprocessed chunk n°53\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.42\n",
      "2.419037103652954\n",
      "Training on preprocessed chunk n°54\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 6.\n",
      "Epoch 8: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.5\n",
      "2.5029401779174805\n",
      "Training on preprocessed chunk n°55\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 10: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.36\n",
      "2.362114667892456\n",
      "Training on preprocessed chunk n°56\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.99\n",
      "2.99052357673645\n",
      "Training on preprocessed chunk n°57\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 7.\n",
      "Epoch 9: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.03\n",
      "2.025953531265259\n",
      "Training on preprocessed chunk n°58\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.69\n",
      "2.694467067718506\n",
      "Training on preprocessed chunk n°59\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.01\n",
      "2.0121119022369385\n",
      "Training on preprocessed chunk n°60\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.36\n",
      "2.3556230068206787\n",
      "Training on preprocessed chunk n°61\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 6: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.51\n",
      "2.5133392810821533\n",
      "Training on preprocessed chunk n°62\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 4.\n",
      "Epoch 6: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 1.74\n",
      "1.7393265962600708\n",
      "Training on preprocessed chunk n°63\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.84\n",
      "2.8363187313079834\n",
      "Training on preprocessed chunk n°64\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 15.\n",
      "Epoch 17: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.26\n",
      "2.262179374694824\n",
      "Training on preprocessed chunk n°65\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 8.\n",
      "Epoch 10: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.5\n",
      "2.503938913345337\n",
      "Training on preprocessed chunk n°66\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.69\n",
      "2.6867597103118896\n",
      "Training on preprocessed chunk n°67\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.08\n",
      "2.0752947330474854\n",
      "Training on preprocessed chunk n°68\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.78\n",
      "2.782910108566284\n",
      "Training on preprocessed chunk n°69\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.31\n",
      "2.309865951538086\n",
      "Training on preprocessed chunk n°70\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.8\n",
      "2.7957775592803955\n",
      "Training on preprocessed chunk n°71\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.19\n",
      "2.1882636547088623\n",
      "Training on preprocessed chunk n°72\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.56\n",
      "2.5596823692321777\n",
      "Training on preprocessed chunk n°73\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.28\n",
      "2.2768735885620117\n",
      "Training on preprocessed chunk n°74\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.05\n",
      "2.0481653213500977\n",
      "Training on preprocessed chunk n°75\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.43\n",
      "2.4261794090270996\n",
      "Training on preprocessed chunk n°76\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.29\n",
      "2.2897958755493164\n",
      "Training on preprocessed chunk n°77\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.19\n",
      "2.1868505477905273\n",
      "Training on preprocessed chunk n°78\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.5\n",
      "2.4991469383239746\n",
      "Training on preprocessed chunk n°79\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.37\n",
      "2.3714230060577393\n",
      "Training on preprocessed chunk n°80\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.79\n",
      "2.7940738201141357\n",
      "Training on preprocessed chunk n°81\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.02\n",
      "2.0205016136169434\n",
      "Training on preprocessed chunk n°82\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.23\n",
      "2.2271316051483154\n",
      "Training on preprocessed chunk n°83\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 3.09\n",
      "3.0881147384643555\n",
      "Training on preprocessed chunk n°84\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.61\n",
      "2.609175682067871\n",
      "Training on preprocessed chunk n°85\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "Epoch 5: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.47\n",
      "2.4745395183563232\n",
      "Training on preprocessed chunk n°86\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "Epoch 7: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.18\n",
      "2.177210807800293\n",
      "Training on preprocessed chunk n°87\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 900 rows with min val MAE: 2.9\n",
      "2.9015164375305176\n",
      "Training on preprocessed chunk n°88\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 3: early stopping\n",
      "✅ Model trained on 342 rows with min val MAE: 3.01\n",
      "3.0095813274383545\n",
      "✅ Trained with MAE: 3.01\n",
      "✅ Results saved locally\n",
      "✅ Model saved locally\n",
      "✅ train() done\n"
     ]
    }
   ],
   "source": [
    "from taxifare.interface.main_local import train\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) 💻 Tensorflow tricks to partial fit without manual chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**📚Resources📚**\n",
    "- tf CSV guide: https://www.tensorflow.org/guide/data#consuming_csv_data\n",
    "- tf CSV tuto: https://www.tensorflow.org/tutorials/load_data/csv\n",
    "- tf Datasets https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/data.ipynb#scrollTo=x5z5B11UjDTd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 16:39:59.351866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, layers, regularizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy paste it below to make it more explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    reg = regularizers.l1_l2(l2=0.005)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Input(shape=(65,)))\n",
    "    model.add(layers.Dense(100, activation=\"relu\", kernel_regularizer=reg))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(50, activation=\"relu\"))\n",
    "    model.add(layers.BatchNormalization(momentum=0.9))  # use momentum=0 to only use statistic of the last seen minibatch in inference mode (\"short memory\"). Use 1 to average statistics of all seen batch during training histories.\n",
    "    model.add(layers.Dropout(rate=0.1))\n",
    "    model.add(layers.Dense(1, activation=\"linear\"))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate= 0.001)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"mae\"])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\",\n",
    "                       patience=2,\n",
    "                       restore_best_weights=True,\n",
    "                       verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) If data fit in memory 😇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.799999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88376</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88377</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88378</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.160004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88379</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88380</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88381 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0    1    2    3    4    5    6    7    8    9   ...   56   57  \\\n",
       "0      0.000000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1      0.142857  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2      0.000000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3      0.142857  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4      0.000000  0.0  0.0  0.0  1.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0   \n",
       "...         ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "88376  0.000000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "88377  0.142857  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "88378  0.142857  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "88379  0.000000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "88380  0.000000  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "        58   59   60   61   62   63   64         65  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0   4.200000  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  11.800000  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  26.600000  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  19.799999  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0   3.400000  \n",
       "...    ...  ...  ...  ...  ...  ...  ...        ...  \n",
       "88376  0.0  0.0  0.0  0.0  0.0  0.0  0.0  14.500000  \n",
       "88377  0.0  0.0  0.0  0.0  0.0  0.0  0.0   9.500000  \n",
       "88378  0.0  0.0  0.0  0.0  0.0  0.0  0.0  74.160004  \n",
       "88379  0.0  0.0  0.0  0.0  0.0  0.0  0.0   8.500000  \n",
       "88380  0.0  0.0  0.0  0.0  0.0  0.0  0.0  16.500000  \n",
       "\n",
       "[88381 rows x 66 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.read_csv(data_processed_path_200k, header=None)\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_small.drop(columns=[65]).to_numpy()\n",
    "target = df_small[[65]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88381, 65)\n",
      "(88381, 1)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(target.shape)\n",
    "n_samples = features.shape[0]\n",
    "n_features = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) passing numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "234/234 [==============================] - 2s 3ms/step - loss: 119.3378 - mae: 8.6492 - val_loss: 66.6020 - val_mae: 6.1894\n",
      "Epoch 2/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 34.1808 - mae: 3.5513 - val_loss: 28.8572 - val_mae: 2.8526\n",
      "Epoch 3/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 23.7866 - mae: 2.5770 - val_loss: 26.9457 - val_mae: 2.7284\n",
      "Epoch 4/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 22.4596 - mae: 2.4575 - val_loss: 24.9422 - val_mae: 2.5978\n",
      "Epoch 5/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 21.3102 - mae: 2.3921 - val_loss: 24.6813 - val_mae: 2.5372\n",
      "Epoch 6/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 20.7079 - mae: 2.3403 - val_loss: 24.3646 - val_mae: 2.5600\n",
      "Epoch 7/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 20.0012 - mae: 2.2991 - val_loss: 23.3986 - val_mae: 2.5130\n",
      "Epoch 8/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 19.7612 - mae: 2.2879 - val_loss: 22.4198 - val_mae: 2.5048\n",
      "Epoch 9/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 19.2913 - mae: 2.2460 - val_loss: 22.3220 - val_mae: 2.5159\n",
      "Epoch 10/10\n",
      "234/234 [==============================] - 1s 3ms/step - loss: 18.8339 - mae: 2.2233 - val_loss: 22.0836 - val_mae: 2.6346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18132ae60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.fit(x=features, y=target, batch_size=BATCH_SIZE, validation_split=0.3, callbacks=[es], epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) passing `datasets` iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((features, target))\n",
    "ds = ds.batch(BATCH_SIZE)  # Set batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 65), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 1), dtype=tf.float64, name=None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([265, 65]), TensorShape([265]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First sample: feature_1, target_1\n",
    "f1, t1 = next(iter(ds))\n",
    "(f1.shape, t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([265, 65]), TensorShape([265, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2, t2 = next(iter(ds))\n",
    "(f2.shape, t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "334/334 [==============================] - 2s 3ms/step - loss: 96.4204 - mae: 7.8560\n",
      "Epoch 2/5\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 26.0324 - mae: 2.7354\n",
      "Epoch 3/5\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 24.2919 - mae: 2.6830\n",
      "Epoch 4/5\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 23.0620 - mae: 2.6041\n",
      "Epoch 5/5\n",
      "334/334 [==============================] - 1s 3ms/step - loss: 22.1436 - mae: 2.5342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181873a30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(ds, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) If data is too large to fit in memory ? 🧐 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Use `make_csv_dataset` helper\n",
    "\n",
    "More info on this tutorial https://www.tensorflow.org/tutorials/load_data/csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The differnce is we dont need to read in all the data in one csv\n",
    "# We can make use of make_csv_dataset to create an iterable of csv's in real time of batch size\n",
    "# We can then fit our model to these smaller csv's without RAM limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 16:41:06.955989: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ds = tf.data.experimental.make_csv_dataset(\n",
    "    data_processed_path_all,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    header=False,\n",
    "    column_names=list(df_small.columns),\n",
    "    label_name=65,\n",
    "    num_epochs=1,\n",
    "    ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now iterate on our dataset `ds` without ever loading all the CSV in memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat1, target1 = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat2, target2 = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the first element (feat1, target1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 target1 is simply a 1D tensor that contains BATCH_SIZE prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target1.shape:  (265,)\n"
     ]
    }
   ],
   "source": [
    "print('target1.shape: ', target1.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "👇 feat1 is a bit more complex, it's an ordered dict that contains N_FEAT=65 elements, each being a BATCH_SIZE = 256 1D vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.OrderedDict'>\n",
      "65\n",
      "(265,)\n"
     ]
    }
   ],
   "source": [
    "print(type(feat1))\n",
    "print(len(feat1))\n",
    "print(feat1[0].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rearrange it as a (BATCH_SIZE, N_FEAT) tensor as we are used to manipulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 65)\n"
     ]
    }
   ],
   "source": [
    "def stack(x):\n",
    "    return tf.stack([x[i] for i in range(65)], axis=1)\n",
    "\n",
    "print(stack(feat1).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now `map` our dataset iterator with this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.map(lambda x,y: (stack(x),y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 65), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 65), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(None,), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.element_spec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it directly to train our model on the **full dataset**! \n",
    "\n",
    "We can train on TB size CSV without RAM limitation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1670/Unknown - 26s 15ms/step - loss: 35.8838 - mae: 3.4139"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m build_model()\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/taxifare-env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.fit(ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
