{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow & Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üè† Import the house price data set. We will keep only numerical features for the sake of simplicity\n",
    "\n",
    "üéØ Your goal will be to fit the best KNN Regressor. In particular, how many \"neighbors\" (<font color=blue>K</font> in <font color=blue>K</font>NN) should you consider to get the best predictions for your house prices ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>163</td>\n",
       "      <td>...</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>142125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>147500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1121 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "Id                                                                            \n",
       "1             60         65.0     8450            7            5       2003   \n",
       "2             20         80.0     9600            6            8       1976   \n",
       "3             60         68.0    11250            7            5       2001   \n",
       "4             70         60.0     9550            7            5       1915   \n",
       "5             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1456          60         62.0     7917            6            5       1999   \n",
       "1457          20         85.0    13175            6            6       1978   \n",
       "1458          70         66.0     9042            7            9       1941   \n",
       "1459          20         68.0     9717            5            6       1950   \n",
       "1460          20         75.0     9937            5            6       1965   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  WoodDeckSF  \\\n",
       "Id                                                      ...               \n",
       "1             2003       196.0         706           0  ...           0   \n",
       "2             1976         0.0         978           0  ...         298   \n",
       "3             2002       162.0         486           0  ...           0   \n",
       "4             1970         0.0         216           0  ...           0   \n",
       "5             2000       350.0         655           0  ...         192   \n",
       "...            ...         ...         ...         ...  ...         ...   \n",
       "1456          2000         0.0           0           0  ...           0   \n",
       "1457          1988       119.0         790         163  ...         349   \n",
       "1458          2006         0.0         275           0  ...           0   \n",
       "1459          1996         0.0          49        1029  ...         366   \n",
       "1460          1965         0.0         830         290  ...         736   \n",
       "\n",
       "      OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  MiscVal  \\\n",
       "Id                                                                            \n",
       "1              61              0          0            0         0        0   \n",
       "2               0              0          0            0         0        0   \n",
       "3              42              0          0            0         0        0   \n",
       "4              35            272          0            0         0        0   \n",
       "5              84              0          0            0         0        0   \n",
       "...           ...            ...        ...          ...       ...      ...   \n",
       "1456           40              0          0            0         0        0   \n",
       "1457            0              0          0            0         0        0   \n",
       "1458           60              0          0            0         0     2500   \n",
       "1459            0            112          0            0         0        0   \n",
       "1460           68              0          0            0         0        0   \n",
       "\n",
       "      MoSold  YrSold  SalePrice  \n",
       "Id                               \n",
       "1          2    2008     208500  \n",
       "2          5    2007     181500  \n",
       "3          9    2008     223500  \n",
       "4          2    2006     140000  \n",
       "5         12    2008     250000  \n",
       "...      ...     ...        ...  \n",
       "1456       8    2007     175000  \n",
       "1457       2    2010     210000  \n",
       "1458       5    2010     266500  \n",
       "1459       4    2010     142125  \n",
       "1460       6    2008     147500  \n",
       "\n",
       "[1121 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load raw data\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/houses_train_raw.csv', index_col=\"Id\")\n",
    "\n",
    "# Only keep numerical columns and raws without NaN\n",
    "data = data.select_dtypes(include=np.number).dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['SalePrice'])\n",
    "y = data['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (Holdout)**‚ùì\n",
    "\n",
    "üëá Split the dataset to create your `X_train` `X_test` and `y_train` `y_test`. Use:\n",
    "- `test_size=0.3`\n",
    "- `random_state=0` to compare your results with your buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚öñÔ∏è Scaling is always crucially important for the KNN algorithm..\n",
    "\n",
    "‚ùì **Question (Scaling)** ‚ùì \n",
    "\n",
    "* Scale your train set and test set.\n",
    "* Here, let's simply apply the `StandardScaler` and not waste time choosing one scaler per feature. Indeed, the goals of this exercise are to:\n",
    "    * review KNN\n",
    "    * understand GridSearchCV\n",
    "    * understand RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train),\n",
    "                              columns = X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.878719</td>\n",
       "      <td>0.186885</td>\n",
       "      <td>-0.066635</td>\n",
       "      <td>-0.159838</td>\n",
       "      <td>1.357895</td>\n",
       "      <td>-0.445886</td>\n",
       "      <td>-1.314462</td>\n",
       "      <td>0.108204</td>\n",
       "      <td>0.326950</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.151130</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>2.195993</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.110711</td>\n",
       "      <td>0.168285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.878719</td>\n",
       "      <td>2.108009</td>\n",
       "      <td>0.117185</td>\n",
       "      <td>-0.871846</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>0.093483</td>\n",
       "      <td>-0.507574</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>0.659783</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352925</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-1.965356</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.878719</td>\n",
       "      <td>-0.453490</td>\n",
       "      <td>-0.533191</td>\n",
       "      <td>-0.871846</td>\n",
       "      <td>2.300009</td>\n",
       "      <td>-0.699707</td>\n",
       "      <td>0.773954</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>-0.065154</td>\n",
       "      <td>0.776043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.117452</td>\n",
       "      <td>0.830288</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>0.260218</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.144006</td>\n",
       "      <td>-0.965790</td>\n",
       "      <td>0.625369</td>\n",
       "      <td>0.552169</td>\n",
       "      <td>-0.526334</td>\n",
       "      <td>0.981856</td>\n",
       "      <td>0.821418</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>2.107377</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854884</td>\n",
       "      <td>0.619980</td>\n",
       "      <td>-0.085080</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>3.490043</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>0.631147</td>\n",
       "      <td>0.913683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.549942</td>\n",
       "      <td>-1.990389</td>\n",
       "      <td>-1.497216</td>\n",
       "      <td>0.552169</td>\n",
       "      <td>-0.526334</td>\n",
       "      <td>1.013584</td>\n",
       "      <td>0.868882</td>\n",
       "      <td>-0.327727</td>\n",
       "      <td>-0.972465</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668006</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.131434</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.481640</td>\n",
       "      <td>-1.322512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>-0.633815</td>\n",
       "      <td>-0.880406</td>\n",
       "      <td>-0.780483</td>\n",
       "      <td>-1.583853</td>\n",
       "      <td>-1.468449</td>\n",
       "      <td>-1.651535</td>\n",
       "      <td>-1.694174</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>-0.402546</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.046103</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>2.246670</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>0.260218</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>0.100898</td>\n",
       "      <td>0.485726</td>\n",
       "      <td>-0.069108</td>\n",
       "      <td>1.264176</td>\n",
       "      <td>-0.526334</td>\n",
       "      <td>0.791490</td>\n",
       "      <td>0.584098</td>\n",
       "      <td>1.585241</td>\n",
       "      <td>1.475907</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>1.905154</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>0.332103</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.110711</td>\n",
       "      <td>-1.322512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>-0.878719</td>\n",
       "      <td>0.400343</td>\n",
       "      <td>-0.203469</td>\n",
       "      <td>-0.159838</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>-0.255521</td>\n",
       "      <td>-1.029678</td>\n",
       "      <td>1.595498</td>\n",
       "      <td>0.288196</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242647</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>1.042860</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>3.974984</td>\n",
       "      <td>-1.223498</td>\n",
       "      <td>-1.322512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>-0.144006</td>\n",
       "      <td>-0.453490</td>\n",
       "      <td>-0.332885</td>\n",
       "      <td>-1.583853</td>\n",
       "      <td>0.415780</td>\n",
       "      <td>-0.572797</td>\n",
       "      <td>-0.649966</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>-0.972465</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214220</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>-0.749484</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>2.690777</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.481640</td>\n",
       "      <td>0.168285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>-0.878719</td>\n",
       "      <td>-0.453490</td>\n",
       "      <td>-0.569461</td>\n",
       "      <td>-0.871846</td>\n",
       "      <td>-0.526334</td>\n",
       "      <td>1.045311</td>\n",
       "      <td>0.963810</td>\n",
       "      <td>-0.584157</td>\n",
       "      <td>1.261617</td>\n",
       "      <td>-0.278554</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300412</td>\n",
       "      <td>-0.738939</td>\n",
       "      <td>0.007627</td>\n",
       "      <td>-0.372566</td>\n",
       "      <td>-0.109954</td>\n",
       "      <td>-0.277928</td>\n",
       "      <td>-0.061673</td>\n",
       "      <td>-0.146085</td>\n",
       "      <td>-0.110711</td>\n",
       "      <td>-0.577113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows √ó 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0     -0.878719     0.186885 -0.066635    -0.159838     1.357895  -0.445886   \n",
       "1     -0.878719     2.108009  0.117185    -0.871846     0.415780   0.093483   \n",
       "2     -0.878719    -0.453490 -0.533191    -0.871846     2.300009  -0.699707   \n",
       "3     -0.144006    -0.965790  0.625369     0.552169    -0.526334   0.981856   \n",
       "4      2.549942    -1.990389 -1.497216     0.552169    -0.526334   1.013584   \n",
       "..          ...          ...       ...          ...          ...        ...   \n",
       "779   -0.633815    -0.880406 -0.780483    -1.583853    -1.468449  -1.651535   \n",
       "780    0.100898     0.485726 -0.069108     1.264176    -0.526334   0.791490   \n",
       "781   -0.878719     0.400343 -0.203469    -0.159838     0.415780  -0.255521   \n",
       "782   -0.144006    -0.453490 -0.332885    -1.583853     0.415780  -0.572797   \n",
       "783   -0.878719    -0.453490 -0.569461    -0.871846    -0.526334   1.045311   \n",
       "\n",
       "     YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n",
       "0       -1.314462    0.108204    0.326950   -0.278554  ...   -1.151130   \n",
       "1       -0.507574   -0.584157    0.659783   -0.278554  ...   -0.352925   \n",
       "2        0.773954   -0.584157   -0.065154    0.776043  ...    1.117452   \n",
       "3        0.821418   -0.584157    2.107377   -0.278554  ...    0.854884   \n",
       "4        0.868882   -0.327727   -0.972465   -0.278554  ...   -0.668006   \n",
       "..            ...         ...         ...         ...  ...         ...   \n",
       "779     -1.694174   -0.584157   -0.402546   -0.278554  ...   -1.046103   \n",
       "780      0.584098    1.585241    1.475907   -0.278554  ...    1.905154   \n",
       "781     -1.029678    1.595498    0.288196   -0.278554  ...   -0.242647   \n",
       "782     -0.649966   -0.584157   -0.972465   -0.278554  ...    0.214220   \n",
       "783      0.963810   -0.584157    1.261617   -0.278554  ...   -0.300412   \n",
       "\n",
       "     WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea  \\\n",
       "0     -0.738939    -0.749484      -0.372566  -0.109954     2.195993 -0.061673   \n",
       "1     -0.738939    -0.749484      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "2      0.830288    -0.749484      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "3      0.619980    -0.085080      -0.372566  -0.109954     3.490043 -0.061673   \n",
       "4     -0.738939    -0.131434      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "..          ...          ...            ...        ...          ...       ...   \n",
       "779   -0.738939    -0.749484       2.246670  -0.109954    -0.277928 -0.061673   \n",
       "780   -0.738939     0.332103      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "781   -0.738939     1.042860      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "782   -0.738939    -0.749484      -0.372566  -0.109954     2.690777 -0.061673   \n",
       "783   -0.738939     0.007627      -0.372566  -0.109954    -0.277928 -0.061673   \n",
       "\n",
       "      MiscVal    MoSold    YrSold  \n",
       "0   -0.146085 -0.110711  0.168285  \n",
       "1   -0.146085 -1.965356 -0.577113  \n",
       "2   -0.146085  0.260218 -0.577113  \n",
       "3   -0.146085  0.631147  0.913683  \n",
       "4   -0.146085 -0.481640 -1.322512  \n",
       "..        ...       ...       ...  \n",
       "779 -0.146085  0.260218 -0.577113  \n",
       "780 -0.146085 -0.110711 -1.322512  \n",
       "781  3.974984 -1.223498 -1.322512  \n",
       "782 -0.146085 -0.481640  0.168285  \n",
       "783 -0.146085 -0.110711 -0.577113  \n",
       "\n",
       "[784 rows x 36 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline KNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (A baseline for our KNN)** ‚ùì\n",
    "\n",
    "Cross-validate (*cv = 5*) a simple KNN regressor taking into account only _the closest neighbor_, and compute the average score over the 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5601542887874071"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=1)\n",
    "cross_validate(knn, X_train_scaled, y_train, cv=5)[\"test_score\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. A first GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (GridSearch v1)**‚ùì\n",
    "\n",
    "Let's use SKLearn `GridSearchCV` to find the best KNN hyperparameter `n_neighbors`.\n",
    "- Start a coarse-grain approach, with `n_neighbors` = [1,5,10,20,50]\n",
    "- 5-fold cross-validate each parameter\n",
    "- Make sure to maximize your performance time using `n_jobs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [1, 5, 10, 20, 50]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [1, 5, 10, 20, 50]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [1, 5, 10, 20, 50]})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instantiate model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# Hyperparameter Grid\n",
    "k_grid = {'n_neighbors' : [1,5,10,20,50]}\n",
    "\n",
    "# Instantiate Grid Search\n",
    "grid = GridSearchCV(model, k_grid, n_jobs=-1,  cv = 5)\n",
    "\n",
    "# Fit data to Grid Search\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (best parameters)** ‚ùì\n",
    "\n",
    "According to the GridSearch, what is the optimal K value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 10}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (scoring)** ‚ùì What is the best score the optimal K value produced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7596697382171873"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. A second GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (GridSearch V2)** ‚ùì\n",
    "\n",
    "\n",
    "Now, we have an idea about where the best $K$ lies, but some of the values we didn't try could result in a  better performance.\n",
    "\n",
    "* Re-run a GridSearch trying some values for $K$ around to your previous best value\n",
    "* What are the `best_score` and `best_k` for this refined GridSearch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "0.7666311417513013\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "\n",
    "# Hyperparameter Grid\n",
    "k_grid = {'n_neighbors' : np.arange(11,30,1)}\n",
    "\n",
    "\n",
    "# Instantiate Grid Search\n",
    "grid = GridSearchCV(model, k_grid, n_jobs=-1,  cv = 5)\n",
    "\n",
    "\n",
    "# Fit data to Grid Search\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(grid.best_params_[\"n_neighbors\"])\n",
    "print(grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_k = grid.best_params_[\"n_neighbors\"]\n",
    "best_score = grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***üß™ Test your code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/bingobango/.pyenv/versions/tom/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/bingobango/code/lewagon/data-workflow/tests\n",
      "plugins: anyio-3.6.1, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_knn.py::TestKnn::test_best_k \u001b[32mPASSED\u001b[0m\u001b[32m                                 [ 50%]\u001b[0m\n",
      "test_knn.py::TestKnn::test_best_score \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.11s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/knn.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed knn step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('knn',\n",
    "                         best_k=best_k,\n",
    "                         best_score=best_score)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Visual check (manual GridSearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è This problem is actually simple enough to perform a GridSearch manually.\n",
    "\n",
    "‚ùì **Question(Manual GridSearch)** ‚ùì\n",
    "\n",
    "- Loop manually over all values of $K$ from $1$ to $50$ and store the average of the cross-validated scores of each model in a list.\n",
    "- Plot the scores as a function of $K$ to visually find the best $K$ using the `Elbow Method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "for k in range(1,50):\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    scores.append(cross_validate(knn, X_train_scaled, y_train, cv=5)[\"test_score\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,50), scores)\n",
    "plt.title(\"Score vs. $K$ Neighbors\")\n",
    "plt.xlabel(\"$K$\")\n",
    "plt.ylabel(\"R2 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCan you guess what makes GridSearchCV a better option than such manual loop ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "- Sklearn's `n_jobs=-1` allows you to parallelize the search, utilizing all of your CPU cores\n",
    "- What if you had multiple hyper-parameters to co-optimize?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GridSearch with multiple parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ KNNRegressor suppports various _distance metrics_ via the hyper-parameter `p` \n",
    "\n",
    "üìö [sklearn.neighbors.KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "‚ùì **Question (tuning multiple parameters)** ‚ùì\n",
    "\n",
    "\n",
    "* Use GridSearchCV to search for the best $K$ and $p$ simultaneously.\n",
    "    * Try all combinations for $K = [1, 5, 10, 20, 50]$ and $p = [1, 2, 3]$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [1, 5, 10, 20, 50], &#x27;p&#x27;: [1, 2, 3]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;n_neighbors&#x27;: [1, 5, 10, 20, 50], &#x27;p&#x27;: [1, 2, 3]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsRegressor(), n_jobs=-1,\n",
       "             param_grid={'n_neighbors': [1, 5, 10, 20, 50], 'p': [1, 2, 3]})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsRegressor()\n",
    "k_grid = {'n_neighbors' : [1, 5, 10, 20, 50],\n",
    "          'p': [1,2,3]}\n",
    "grid = GridSearchCV(model, k_grid,\n",
    "                     cv = 5, n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (number of submodels)**‚ùì\n",
    "\n",
    "How many submodels did you train overall?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "Much more than 15. Think twice :)\n",
    "    <details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "75 models due to CV=5\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (best parameters and best score after tuning the model with multiple parameters)**‚ùì\n",
    "\n",
    "What are the *best parameters* and the *best score*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 10, 'p': 1}\n",
      "0.7969255879201194\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see whether a RandomizedSearch can find a better combination with the same number of models being fitted.\n",
    "\n",
    "‚ùì **Question (RandomizedSearchCV)** ‚ùì\n",
    "\n",
    "Use `RandomizedSearchCV` to\n",
    "- Randomly sample $K$ from a uniform `scipy.stats.randint(1,50)` ([doc](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.randint.html)) distribution\n",
    "- Sample $p$ from a list $[1,2,3]$\n",
    "- Use the correct numbers of `n_iter` and `cv` to fit the exact same numbers of models as in your previous GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7949980062278728\n",
      "{'n_neighbors': 7, 'p': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "search_space = {'n_neighbors': randint(1, 50), 'p': [1, 2, 3]}\n",
    "\n",
    "search = RandomizedSearchCV(model, param_distributions=search_space,\n",
    "                            n_jobs=-1,  cv=5, n_iter=15)\n",
    "\n",
    "search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (finetuning your model one more time)**‚ùì\n",
    "\n",
    "- Refine your RandomsearchCV if you want\n",
    "- Choose your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsRegressor(n_neighbors=7, p=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(n_neighbors=7, p=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsRegressor(n_neighbors=7, p=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = search.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to display your `cv_results` as a `DataFrame`, this will help you visualize what's going on inside the CV! üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>param_p</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005049</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.156426</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 44, 'p': 3}</td>\n",
       "      <td>0.701537</td>\n",
       "      <td>0.766204</td>\n",
       "      <td>0.646848</td>\n",
       "      <td>0.737823</td>\n",
       "      <td>0.657637</td>\n",
       "      <td>0.702010</td>\n",
       "      <td>0.045641</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.060285</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 7, 'p': 1}</td>\n",
       "      <td>0.736660</td>\n",
       "      <td>0.817465</td>\n",
       "      <td>0.757978</td>\n",
       "      <td>0.867009</td>\n",
       "      <td>0.795878</td>\n",
       "      <td>0.794998</td>\n",
       "      <td>0.045749</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.026256</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 31, 'p': 2}</td>\n",
       "      <td>0.736305</td>\n",
       "      <td>0.798639</td>\n",
       "      <td>0.695717</td>\n",
       "      <td>0.784442</td>\n",
       "      <td>0.723642</td>\n",
       "      <td>0.747749</td>\n",
       "      <td>0.038355</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.011446</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 33, 'p': 1}</td>\n",
       "      <td>0.762959</td>\n",
       "      <td>0.802696</td>\n",
       "      <td>0.718433</td>\n",
       "      <td>0.804131</td>\n",
       "      <td>0.735004</td>\n",
       "      <td>0.764644</td>\n",
       "      <td>0.034710</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 26, 'p': 2}</td>\n",
       "      <td>0.738524</td>\n",
       "      <td>0.811411</td>\n",
       "      <td>0.699374</td>\n",
       "      <td>0.800078</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.756538</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.097591</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 6, 'p': 3}</td>\n",
       "      <td>0.675765</td>\n",
       "      <td>0.754340</td>\n",
       "      <td>0.667509</td>\n",
       "      <td>0.844996</td>\n",
       "      <td>0.669192</td>\n",
       "      <td>0.722360</td>\n",
       "      <td>0.069382</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 29, 'p': 2}</td>\n",
       "      <td>0.738281</td>\n",
       "      <td>0.802296</td>\n",
       "      <td>0.696320</td>\n",
       "      <td>0.789035</td>\n",
       "      <td>0.728506</td>\n",
       "      <td>0.750888</td>\n",
       "      <td>0.039334</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 41, 'p': 1}</td>\n",
       "      <td>0.753112</td>\n",
       "      <td>0.797950</td>\n",
       "      <td>0.715240</td>\n",
       "      <td>0.789323</td>\n",
       "      <td>0.722191</td>\n",
       "      <td>0.755564</td>\n",
       "      <td>0.033711</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.091727</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 17, 'p': 3}</td>\n",
       "      <td>0.725889</td>\n",
       "      <td>0.788035</td>\n",
       "      <td>0.705242</td>\n",
       "      <td>0.776904</td>\n",
       "      <td>0.695673</td>\n",
       "      <td>0.738348</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.004554</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 37, 'p': 2}</td>\n",
       "      <td>0.731045</td>\n",
       "      <td>0.789914</td>\n",
       "      <td>0.688473</td>\n",
       "      <td>0.776315</td>\n",
       "      <td>0.708478</td>\n",
       "      <td>0.738845</td>\n",
       "      <td>0.038813</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.089074</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 7, 'p': 3}</td>\n",
       "      <td>0.678143</td>\n",
       "      <td>0.766732</td>\n",
       "      <td>0.662546</td>\n",
       "      <td>0.831729</td>\n",
       "      <td>0.675597</td>\n",
       "      <td>0.722949</td>\n",
       "      <td>0.065800</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.008158</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_neighbors': 18, 'p': 1}</td>\n",
       "      <td>0.760167</td>\n",
       "      <td>0.806875</td>\n",
       "      <td>0.732038</td>\n",
       "      <td>0.834326</td>\n",
       "      <td>0.770070</td>\n",
       "      <td>0.780695</td>\n",
       "      <td>0.035954</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.085160</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 12, 'p': 3}</td>\n",
       "      <td>0.714020</td>\n",
       "      <td>0.775418</td>\n",
       "      <td>0.698739</td>\n",
       "      <td>0.794063</td>\n",
       "      <td>0.693504</td>\n",
       "      <td>0.735149</td>\n",
       "      <td>0.041470</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002371</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 26, 'p': 2}</td>\n",
       "      <td>0.738524</td>\n",
       "      <td>0.811411</td>\n",
       "      <td>0.699374</td>\n",
       "      <td>0.800078</td>\n",
       "      <td>0.733302</td>\n",
       "      <td>0.756538</td>\n",
       "      <td>0.042518</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.069674</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_neighbors': 4, 'p': 3}</td>\n",
       "      <td>0.660395</td>\n",
       "      <td>0.741452</td>\n",
       "      <td>0.660902</td>\n",
       "      <td>0.845153</td>\n",
       "      <td>0.636259</td>\n",
       "      <td>0.708832</td>\n",
       "      <td>0.076887</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.005049      0.000112         0.156426        0.001295   \n",
       "1        0.004953      0.000023         0.060285        0.001685   \n",
       "2        0.003847      0.000919         0.026256        0.024975   \n",
       "3        0.003091      0.000146         0.011446        0.000666   \n",
       "4        0.002985      0.000100         0.005188        0.000238   \n",
       "5        0.002769      0.000145         0.097591        0.002357   \n",
       "6        0.002902      0.000336         0.005033        0.000199   \n",
       "7        0.003146      0.000105         0.010630        0.001003   \n",
       "8        0.002528      0.000310         0.091727        0.003611   \n",
       "9        0.002340      0.000142         0.004554        0.000233   \n",
       "10       0.002533      0.000217         0.089074        0.001533   \n",
       "11       0.002263      0.000135         0.008158        0.000121   \n",
       "12       0.002400      0.000171         0.085160        0.002361   \n",
       "13       0.002371      0.000114         0.004301        0.000185   \n",
       "14       0.002533      0.000195         0.069674        0.003036   \n",
       "\n",
       "   param_n_neighbors param_p                       params  split0_test_score  \\\n",
       "0                 44       3  {'n_neighbors': 44, 'p': 3}           0.701537   \n",
       "1                  7       1   {'n_neighbors': 7, 'p': 1}           0.736660   \n",
       "2                 31       2  {'n_neighbors': 31, 'p': 2}           0.736305   \n",
       "3                 33       1  {'n_neighbors': 33, 'p': 1}           0.762959   \n",
       "4                 26       2  {'n_neighbors': 26, 'p': 2}           0.738524   \n",
       "5                  6       3   {'n_neighbors': 6, 'p': 3}           0.675765   \n",
       "6                 29       2  {'n_neighbors': 29, 'p': 2}           0.738281   \n",
       "7                 41       1  {'n_neighbors': 41, 'p': 1}           0.753112   \n",
       "8                 17       3  {'n_neighbors': 17, 'p': 3}           0.725889   \n",
       "9                 37       2  {'n_neighbors': 37, 'p': 2}           0.731045   \n",
       "10                 7       3   {'n_neighbors': 7, 'p': 3}           0.678143   \n",
       "11                18       1  {'n_neighbors': 18, 'p': 1}           0.760167   \n",
       "12                12       3  {'n_neighbors': 12, 'p': 3}           0.714020   \n",
       "13                26       2  {'n_neighbors': 26, 'p': 2}           0.738524   \n",
       "14                 4       3   {'n_neighbors': 4, 'p': 3}           0.660395   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.766204           0.646848           0.737823   \n",
       "1            0.817465           0.757978           0.867009   \n",
       "2            0.798639           0.695717           0.784442   \n",
       "3            0.802696           0.718433           0.804131   \n",
       "4            0.811411           0.699374           0.800078   \n",
       "5            0.754340           0.667509           0.844996   \n",
       "6            0.802296           0.696320           0.789035   \n",
       "7            0.797950           0.715240           0.789323   \n",
       "8            0.788035           0.705242           0.776904   \n",
       "9            0.789914           0.688473           0.776315   \n",
       "10           0.766732           0.662546           0.831729   \n",
       "11           0.806875           0.732038           0.834326   \n",
       "12           0.775418           0.698739           0.794063   \n",
       "13           0.811411           0.699374           0.800078   \n",
       "14           0.741452           0.660902           0.845153   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.657637         0.702010        0.045641               15  \n",
       "1            0.795878         0.794998        0.045749                1  \n",
       "2            0.723642         0.747749        0.038355                8  \n",
       "3            0.735004         0.764644        0.034710                3  \n",
       "4            0.733302         0.756538        0.042518                4  \n",
       "5            0.669192         0.722360        0.069382               13  \n",
       "6            0.728506         0.750888        0.039334                7  \n",
       "7            0.722191         0.755564        0.033711                6  \n",
       "8            0.695673         0.738348        0.037491               10  \n",
       "9            0.708478         0.738845        0.038813                9  \n",
       "10           0.675597         0.722949        0.065800               12  \n",
       "11           0.770070         0.780695        0.035954                2  \n",
       "12           0.693504         0.735149        0.041470               11  \n",
       "13           0.733302         0.756538        0.042518                4  \n",
       "14           0.636259         0.708832        0.076887               14  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (Evaluation of the \"best\" model)** ‚ùì\n",
    "\n",
    "* Time has come to discover our model's performance with \"best params\" on the **unseen** test set `X_test`.\n",
    "    * Compute the r2 score for the test set and save it as `r2_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bingobango/.pyenv/versions/3.10.6/envs/tom/lib/python3.10/site-packages/sklearn/base.py:450: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7690584913149612"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_test = r2_score(y_test, best_model.predict(scaler.transform(X_test)))\n",
    "r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question (Taking a step back)** ‚ùì\n",
    "\n",
    "Would you consider the optimized model to generalize well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Answer</summary>\n",
    "\n",
    "Test score may decrease a bit with train set. Probably not more than 5%. This can be due to\n",
    "- A non-representative train/test split\n",
    "- A cross-val number too small leading to overfitting the model-tuning phase. The more you cross-validated, the more robust your findings will generalize - but you can't increase cv too much if your dataset is too small as you won't keep enough observations in each fold to be representative.\n",
    "- Our dataset is very small and our hyperparameter optimization is thus extremely dependent (and overfitting) on our train/test split. Always make sure your dataset is much bigger than the total number of hyperparameter combinations you are trying out!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***üß™ Test your code***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/bingobango/.pyenv/versions/tom/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/bingobango/code/lewagon/data-workflow/tests\n",
      "plugins: anyio-3.6.1, asyncio-0.19.0, typeguard-2.13.3\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_r2.py::TestR2::test_r2 \u001b[32mPASSED\u001b[0m\u001b[32m                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.08s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/r2.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed r2 step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('r2', \n",
    "                         r2_test=r2_test)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! Now, you know how to finetune a model using either a GridSearchCV or a RandomizedSearchCV \n",
    "\n",
    "üíæ Don't forget to¬†`git add/commit/push`¬†your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
